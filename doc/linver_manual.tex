\documentclass{book}
\usepackage{amsmath, amsthm, amssymb, amsfonts, amssymb}
\usepackage{fullpage, graphicx, graphics}
\usepackage{cite}


% Use to prevent indenting paragraphs
%\setlength{\parindent}{0in}
%\setlength{\parskip}{8pt plus 1pt minus 1pt}

% Remove blank pages between chapters
%\let\cleardoublepage\clearpage

% All this controls the margins - change if you want
%\oddsidemargin=0.10in
%\evensidemargin=0.10in
%\textwidth=6in
%\topmargin=0in
%\textheight=9in
% Header stuff
%\headheight=0in
%\headsep=0in


\title{LinVer Manual}
%\author{Jerry Mcmahan}
\begin{document}

\maketitle
%\tableofcontents

LinVer is a reference Matlab implementation of a verification framework for Bayesian inference algorithms 
outlined
in \cite{adams01}. It is
based on a linear regression problem for which analytical or semi-analytical solutions are known.  It provides a 
rigorous means of testing output chains of Markov chain Monte Carlo (MCMC) algorithms used for 
Bayesian inference are distributed correctly
via an implementation of a hypothesis test for equal distributions based on the energy distance
statistic \cite{szekely01}. While the main goal of the code is as a reference for those interested in 
implementing the framework in other verification software, it is also useable as-is as a basic verification
tool. Mathematical details of the framework can be found in Appendix~A of \cite{adams01} and will
be described further in a forthcoming paper. 

Be aware that LinVer is in development and may contain some bugs. The implementation
of the energy statistic test is not completely verified. The calculation of the true posteriors is believed
to be correct, however, for the basic cases. 

\chapter{Parameters of the Linear Equation}
\label{chap:lineqn}
The verification framework is based on Bayesian inference for the linear equation defined by
\begin{align*}
y = G\beta + \varepsilon(\lambda, \phi).
\end{align*}
The variables making up this equation are:
\begin{align*}
y &: \text{ A vector of N observations.} \\
G &: \text{ The } N \times N_\beta \text{ design matrix.} \\
\beta &: \text{ The } N_\beta \times 1 \text{ column vector } 
	     \begin{bmatrix}\beta_1 & \cdots & \beta_{N_\beta}\end{bmatrix}^T 
	     \text{ of regression parameters.} \\
\varepsilon &: \text{ The } N \times 1 \text{ column vector of observation noise.} \\
\lambda &: \text{ The scale parameter for the noise.} \\
\phi &: \text{ The correlation parameter for the noise covariance.}
\end{align*}
In this implementation of the framework, the first column of the design matrix $G$ is all one's, so the first
regression parameter $\beta_1$ is a bias term. The remaining entries of $G$ (if any) are drawn from a
standard normal distribution (i.e., mean 0 and variance 1). This choice of $G$ is not crucial to the framework
as-implemented and can be substituted. 

The observation noise is zero-mean and normally distributed so that $\varepsilon \sim N\left(0, C\right)$
with the $N \times N$ covariance matrix $C(\lambda, \phi)$ depending on the parameters $\lambda, \phi$. The 
$\lambda,\phi$ dependence is expressed by
\begin{align*}
C(\lambda, \phi) = \frac{1}{\lambda}R(\phi),
\end{align*}
where $R(\phi)$ is the correlation function which depends on $\phi$. The framework includes three possible choices
of correlation:
\begin{align*}
\text{No correlation: } R(\phi) &= 
\begin{bmatrix} 
	1 & 0 & 0 & \cdots & 0 \\
	0 & 1 & 0 & \cdots & 0 \\
	0 & 0 & 1 & \cdots & 0 \\
	\vdots & \vdots & \vdots & \ddots & \vdots \\
	0 & 0 & 0 & \cdots & 1	
\end{bmatrix} \\
\text{Equicorrelation: } R(\phi) &= 
\begin{bmatrix}
	1    & \phi & \phi & \cdots  &\phi &  \phi \\
	\phi & 1    & \phi & \cdots  & \phi & \phi \\
	\phi & \phi    & 1 & \cdots  & \phi & \phi \\
	\vdots & \vdots & \vdots  & \ddots & \vdots & \vdots \\
	\phi & \phi & \phi & \cdots & 1 & \phi \\
	\phi & \phi & \phi & \cdots & \phi & 1 	
\end{bmatrix}\\
\text{AR(1) correlation: } R(\phi) &= 
\begin{bmatrix}
	1    & \phi & \phi^2 & \cdots  &\phi^{N-2} &  \phi^{N-1} \\
	\phi & 1    & \phi & \cdots  & \phi^{N-3} & \phi^{N-2} \\
	\phi^2 & \phi    & 1 & \cdots  & \phi^{N-4} & \phi^{N-3} \\
	\vdots & \vdots & \vdots  & \ddots & \vdots & \vdots \\
	\phi^{N-2} & \phi^{N-3} & \phi^{N-4} & \cdots & 1 & \phi \\
	\phi^{N-1} & \phi^{N-2} & \phi^{N-3} & \cdots & \phi & 1 	
\end{bmatrix}\\
\end{align*}
For each of the correlation cases, the correlation parameter $\phi$ is restricted to a different domain:
\begin{align*}
\text{No correlation: } &\phi \in \emptyset\\
\text{Equicorrelation: } &\phi \in [0, 1)\\
\text{AR(1) correlation: } &\phi \in (-1, 1)
\end{align*} 
Note that although we have defined the domain for $\phi$ in the no correlation case to be the empty set since
the corresponding correlation matrix in this case has no $\phi$ dependence. The reason for using these choices
of covariance functions is that these choices each have known analytical forms for the inverse and determinant.
This facilitates computation of the exact solution to the Bayesian inverse problems described below, as the inverse
and determinant of the covariance matrices appear in the analytical and semi-analytical solutions for the parameter
distributions. 

\chapter{Bayesian Inference}
The purpose of the framework is to provide a means of verifying Bayesian inference algorithms using the
equations and parameters in Chapter~\ref{chap:lineqn}. The framework has three different cases of unknown
parameters depending on which of the parameters $\beta, \lambda,$ and $\phi$ are treated as unknown. 
The cases used in the framework are summarized in the following table.

\vspace{.1in}
\centerline{
\def\arraystretch{1.5}
\begin{tabular} {| c | c | c | c |}
\hline
Case & Known & Unknown & Calibration Parameters \\
\hline
1 & $\lambda, \phi$ & $\beta$ & $\hat \beta$ \\
2 & $\phi$ & $\beta, \lambda$ & $\hat \beta, \hat \lambda$ \\
3 & \text{None} & $\beta, \lambda, \phi$ & $\hat \beta, \hat \lambda, \hat \phi$ \\
\hline
\end{tabular}
}

\vspace{.1in}
Bayesian inference relies on Bayes rule
\begin{align*}
p\left(\left. \hat \theta \right| y  \right) = \frac{p\left( y \left| \hat \theta \right. \right) p_0\left(\hat \theta\right)} { p\left(y\right)},
\end{align*}
where $\hat \theta$ is the inferred parameter, $y$ is the data, and the $p(\cdot)$ functions refer to
the following likelihoods:
\begin{align*}
p\left(\left. \hat \theta \right| y \right) &: \text{ Posterior of parameter value } \hat \theta \text{ given data } y \\
p\left(\left. \hat \theta \right| y \right) &: \text{ Likelihood of data } y \text{ given parameter value } \hat \theta \\
p_0\left(\hat \theta\right) &: \text{ Prior for inferred parameter value } \hat \theta \\
p\left(y\right) &: \text{ Marginal likelihood of the data } y \text{ given by }
 \int_{D(\hat \theta)} p\left( y \left| \hat \theta \right. \right) p_0\left(\hat \theta\right)\,d\hat\theta
\end{align*}

As can be determined from the above table and Bayes rule above, the verification framework computes the following
marginal posteriors according to which set of parameters are chosen to be calibrated.
\begin{align*}
\textbf{Case 1} &: p\left(\left. \hat \beta \right| y\right)\\
\textbf{Case 2} &: p\left(\left. \hat \beta \right| y\right), \;\; p\left(\left. \hat \lambda \right| y\right) \\
\textbf{Case 3} &: p\left(\left. \hat \beta \right| y\right), \;\; p\left(\left. \hat \lambda \right| y\right), \;\; p\left(\left. \hat \phi \right| y\right) \\
\end{align*}
Case~1 and Case~2 are computed exactly, in the sense that an exact parametric distribution for each of the marginal
posteriors can be derived which can be computed using standard routines for calculating the associated special functions.
The posterior for Case~3 is an integral expression involving special functions and is computed using numerical integration. 
Note that Case~3 requires the use of the equicorrelation or AR(1) correlation function, since there is no $\phi$ dependence
for uncorrelated observation error. 

For each of the three cases of unknown parameters, there are two choices of prior. These are labeled the ``Non-informative"
and ``Gaussian" priors, according to whether the marginal prior for $\hat \beta$ is uniform or Gaussian. The following describes
the priors for each of the calibrated variables.

\vspace{.1in}
\centerline{\textbf{Non-informative}}
\begin{align*}
\textbf{Case 1}: &\;\;p_0\left(\hat \beta\right) \propto 1
\\
\textbf{Case 2}: &\;\;p_0\left(\left. \hat \beta \right| \hat \lambda \right) \propto 1\\ 
			&\;\;p_0\left(\hat \lambda \right) \propto \frac{1}{\hat \lambda}
\\
\textbf{Case 3}: &\;\;p_0\left(\left. \hat \beta \right| \hat \lambda, \hat \phi \right) \propto 1 \\
			&\;\;p_0\left(\left. \hat \lambda \right| \hat \phi \right) \propto \frac{1}{\hat \lambda} \\
			&\;\;p_0\left(\hat \phi\right) = \frac{1}{\phi_H - \phi_L}
\end{align*}
The priors which are described as a proportional expression are improper priors. The parameters $\phi_H$ and
$\phi_L$ are the high and low values of the domain of $\phi$, so that $\phi \in [\phi_L, \phi_H]$. These are configurable
parameters in the framework and will depend on the choice of correlation function. These are the only parameters
needed for the non-informative prior, and only when $\phi$ is an unknown parameter (i.e., only in Case~3). 


\vspace{.1in}
\centerline{\textbf{Gaussian}}
\begin{align*}
\textbf{Case 1}: &\;\;p_0\left(\hat \beta\right) \text{ is } N\left(\mu_0, \frac{1}{\lambda} \Sigma_0\right)
\\
\textbf{Case 2}: &\;\;p_0\left(\left. \hat \beta \right| \hat \lambda \right)  \text{ is } N\left(\mu_0, \frac{1}{\hat \lambda} \Sigma_0\right)\\ 
			&\;\;p_0\left(\hat \lambda \right) \propto \frac{1}{\hat \lambda}
\\
\textbf{Case 3}: &\;\;p_0\left(\left. \hat \beta \right| \hat \lambda, \hat \phi \right)   \text{ is } N\left(\mu_0, \frac{1}{\hat \lambda} \Sigma_0\right) \\
			&\;\;p_0\left(\left. \hat \lambda \right| \hat \phi \right) \propto \frac{1}{\hat \lambda} \\
			&\;\;p_0\left(\hat \phi\right) = \frac{1}{ \phi_H - \phi_L}
\end{align*}
The only difference between the non-informative and Gaussian cases is the prior on $\hat \beta$ is Gaussian with a
prescribed $N_\beta \times 1$ mean vector $\mu_0$ and unscaled $N_\beta \times N_\beta$ covariance $\Sigma_0$. The
framework uses diagonal $\Sigma_0$, although this is not strictly necessary. Make note that the prior distribution in 
Case~1 has a covariance matrix scaled by $1 / \lambda$, the known value of $\lambda$. In the other cases $\lambda$ is
unknown so this scaling is replaced by $1 / \hat \lambda$, the calibrated variable. 

\chapter{Using the Verification Framework}
The following is an outline of the steps involved in using the verification framework.
\begin{enumerate}
\item Choose the size of the problem: $N$, the number of observations and $N_\beta$, the number of
	regression parameters. 
\item Choose the values of the true parameters $\beta, \lambda, \phi$. These are the values that the Bayesian inference
	algorithm will attempt to learn from the data. 
\item Set the design matrix $G$. The default recommendation (and what is used in examples) is to set all entries in the first
	column to ones and fill the remaining entries (if any) with data drawn from a standard normal distribution.
\item Use the true parameter $\beta$ to determine the error-free model output $y_0$ according to $y_0 = G\beta$. 
\item Choose a correlation function $R(\phi)$: no correlation, equicorrelation, or AR(1) correlation. 
\item Use the chosen correlation function and the true parameters $\lambda, \phi$ to generate the observation error
	$\varepsilon$ according to the distribution $N(0, C)$ where $C = R(\phi) / \lambda$. 
\item Add the observation error $\varepsilon$ to the error-free model output $y_0$ to obtain the calibration data 
	$y = y_0 + \varepsilon$. 
\item Choose which parameters to calibrate (i.e., which are treated as unknowns): 
	Case~1: $\hat \beta$, 
	Case~2: $\hat\beta, \hat \lambda$, or
	Case~3: $\hat\beta, \hat \lambda, \hat \phi$. If using Case~3, choose the parameters $\phi_L, \phi_H$ defining
	restricting the domain of $\phi$. These need to be chosen in agreement with the domain associated with the
	correlation function. 
\item Choose whether to use the non-informative prior or the Gaussian prior for the regression parameters $\hat \beta$.
	If using the Gaussian prior, choose the prior mean $\mu_0$ and prior un-scaled covariance $\Sigma_0$.
\item Using the calibration routine to be verified, calculate the posterior for the unknown parameters. It is important that this 
	be done using the calibration data $y$ generated above, using the same prior, and also using the same design
	matrix $G$ set above.
\item Use the verification framework to calculate the true posterior using all the chosen parameters as input.
\item Compare the true posterior computed by the verification framework with that computed by the routine being verified.
\end{enumerate}

This outline leaves open the specific ways the comparison between the true posterior and the output of the calibration
routine are to be done. There are multiple ways to do this, some being more appropriate for different types of calibration
routines. We specifically mention the case of verifying Markov chain Monte Carlo (MCMC) output, as we have included 
a method aimed at this common case. Here we recommend a statistical hypothesis test based on the energy statistic, and 
the Matlab implementation includes an implementation of this.

\chapter{Using the Matlab Implementation}
The Matlab scripts {\tt demo\_case1.m, demo\_case2.m, demo\_case3.m} demonstrate the use of the Matlab implementation
for Case~1 ($\beta$ unknown), Case~2 ($\beta, \lambda$ unknown), and Case~3 ($\beta, \lambda, \phi$ unknown) respectively.

The main function used in computing the posterior is {\tt eval\_posterior}. This function takes a structure {\tt param} as input
which defines all the parameters of the problem. The fields of this structure are as follows.

\vspace{.1in}
\begin{tabular}{l l}
{\tt param.N}: & Number of observations. \\
{\tt param.Nbeta}: & Number of regression parameters. \\
{\tt param.G}:  & {\tt N} by {\tt Nbeta} design matrix. \\
{\tt param.prior}: & Structure defining the prior. See below for more information. \\
{\tt param.beta}: & {\tt Nbeta} by 1 vector of the true regression parameters $\beta$ \\
{\tt param.lambda}: & Scalar value of the true scale parameter $\lambda$. \\
{\tt param.phi}: & Scalar value of the true correlation parameter $\phi$. \\  
{\tt param.corrfunc}: & String defining the correlation function type: {\tt "none", "equal", "ar"}. \\
{\tt param.y}: & {\tt N} by 1 vector of observations. \\
{\tt param.unknowns}: & String defining which parameters are unknown. See below. \\
{\tt param.betarange}: & {\tt Nbeta} by 2 matrix. Left column is lower limit of $\beta$, right column is upper limit. \\
{\tt param.lambdarange}: & 1 by 2 matrix. Left value is lower limit of $\lambda$, right column is upper limit. \\
{\tt param.phirange}: & 1 by 2 matrix. Left value is lower limit of $\phi$, right column is upper limit. \\
\end{tabular}

\vspace{.1in}
\textbf{Prior structure}. Let the prior structure be named {\tt prior}. It always has the field {\tt prior.type} which is a string variable with
two possible values: {\tt "noninformative"} or {\tt gaussian}. If the {\tt prior.type} field is set to {\tt "noninformative"}, no other
fields need to be defined. If it is set to {\tt "gaussian"}, the field {\tt prior.sigma0} must be the {\tt Nbeta} by {\tt Nbeta}
unscaled covariance and {\tt prior.mu0} is the {\tt Nbeta} by 1 mean vector of the prior. 

\vspace{.1in}
\textbf{Unknowns strings}. The {\tt prior.unknowns} field can take one of the string values:
{\tt "beta"}, {\tt"beta\_lambda"}, {\tt"beta\_lambda\_phi"}. These correspond to whether the unknowns are Case~1: $\beta$, 
Case~2: $\beta, \lambda$, or Case~3: $\beta,\lambda,\phi$. 

\vspace{.1in}
\textbf{Calibration data}. The function {\tt eval\_noise} takes the same {\tt param} structure above as input, except {\tt param.y}
need not be defined. It returns a {\tt N} by 1 vector of unscaled observation error. This vector can be divided by $\sqrt{\lambda}$
and added to the error-free model output to provide calibration data.

\vspace{.1in}
\textbf{Range parameters}. The {\tt param.betarange}, {\tt param.lambdarange}, and {\tt param.phirange} fields are defined
for practical purposes to bound the range of the parameters. In the ideal problem, each component of $\beta$ can be
any component in $\mathbb{R}$ and $\lambda$ ranges from $(0, \infty)$. The ranges for these parameters are used only by the 
Metropolis-Hastings algorithm provided as an example routine for verification and should not affect the true posterior calculation.
The range for $\phi$, however, is different, as this defines range of the uniform prior. It is necessary to choose this prior so that
the range is smaller than the range of the actual correlation parameter ($[0,1)$ for equicorrelation and $(-1, 1)$ for AR(1) 
correlation) as the posterior for $\phi$ involves integrals which do not converge if every point in the domain of $\phi$ is 
weighted by a uniform prior. 

\vspace{.1in}
\textbf{Posterior structure}. The function {\tt eval\_posterior} returns a structure which we call {\tt post}. The fields of
this structure depend on which parameters are unknown. The posterior structure always includes a field defining
a function for the marginal posterior density of each unknown. These are given by {\tt post.pbeta},
{\tt post.plambda}, and {\tt post.pphi}. These functions are used by passing a vector argument of values to evaluate
the posterior at. For example, the code

\vspace{.1in}
 {\tt beta = linspace(-1,1)'; y = post.pbeta(beta);} 
\vspace{.1in}

\noindent evaluates the posterior for a scalar-valued $\beta$ at 100 evenly spaced points between -1 and 1 and stores the result
in the variable {\tt y}. If $N_\beta > 1$ then the argument should be an $M \times N_\beta$ vector, where $M$ is the
number of points in parameter space to evaluate the posterior of $\beta$ at. The functions for the posterior of
$\lambda$ and $\phi$ work similarly, except there is no need to consider a vector-valued case since these are always
scalar-valued random variables. 

In addition to these functions, in the cases where the posteriors are known parametric distributions, relevant
parameters are also returned. In Case~1, $\beta$ is unknown and its posterior is Gaussian with mean $\mu_2$
and covariance $\Sigma_2 / \lambda$ (where $\lambda$ is the known scale parameter). These are returned in the
posterior as {\tt post.mu2} and {\tt post.sigma2}. In Case~2, $\beta$ and $\lambda$ are both unknown. The posterior
of $\lambda$ is a Gamma distribution with shape parameter $a_1$ and scale parameter $b_1$. These are returned
as {\tt post.a1} and {\tt post.b1}. Note that there are multiple conventions for the parameterization of the Gamma
distribution. The convention used here is compatible with that used in Matlab if {\tt 1 / post.b1} is used for the scale
parameter. The posterior for $\beta$ is a multivariate $t$-distribution with a scale matrix, location vector, and 
degrees-of-freedom parameter. These are returned as {\tt post.scl}, {\tt post.loc}, and {\tt post.dof}, respectively. 
The marginals for Case~3 are not known parametric distributions, so we will not discuss the other fields defined.

\vspace{.1in}
\textbf{Energy statistic test for equal distributions.} An implementation of a non-parametric test of distribution equality
for two samples is included in the Matlab code. This provides means of comparing a random sample drawn from the
posterior with a given sample computed from a Markov chain Monte Carlo code, for example. The function is

\vspace{.1in}
{\tt results = do\_energy\_test(sample, param, post, alpha, numtests, N)}

\vspace{.1in}
\noindent where the arguments are: 

\vspace{.1in}
{\tt sample}: Sample to compare to the true posterior. This should be drawn from the code being verified. It should be
a $M \times N_{param}$ matrix of $M$ observations of the $N_{param}$-dimensional parameter vector. That is, each
row is an observation of the parameter vector, whose size $N_{param}$ depends on which parameters are unknown. 
For Case~1 $N_{param} = N_\beta$ and each observation is the regression parameters. For Case~2 
$N_{param} = N_\beta + 1$ and each observation is the $N_\beta$ regression parameter observations followed by the 
$\lambda$ observation. For Case~3 $N_{param} = N_\beta + 2$ and each observation is the $N_\beta$ regression 
parameters, followed by the $\lambda$ observation, followed by the $\phi$ observation. 

{\tt param}: This is the parameter structure described above.

{\tt post}: This is the posterior structure returned by {\tt eval\_posterior}.

{\tt alpha}: This is the confidence level of the test, between 0 and 1. Lower values represent stricter confidences, but
will lead to more computationally involved tests. Defaults to 0.05. 

{\tt numtests}: This is the number of hypothesis tests to run. Each test draws a separate set of realizations from the
posterior to use in the hypothesis test which checks if {\tt sample} is drawn from the same distribution. The number
of failed tests significantly exceeds {\tt numtests * alpha}, then it is reasonable to expect that {\tt sample}
comes from a different distribution and the code that generated it failed to produce the correct output at the given
signficance level. Defaults to 100.

{\tt N}: Number of samples to draw from the true posterior to compare with {\tt sample}. Defaults to 100. 

If the arguments after {\tt post} are omitted, the defaults described above will be used. The {\tt results} structure
has the following fields. 

\vspace{.1in}
{\tt results.exactsamp}: This is a {\tt N * numtests} by {\tt Nparam} matrix containing the samples drawn from the
true posterior for each test. 

{\tt results.fail\_ratio}: This is the ratio of tests run which were rejected. If this value exceeds {\tt alpha} significantly,
the correctness of {\tt sample} is questionable. 

\vspace{.1in}
\textbf{Sampling from the true posterior.} The function {\tt draw\_posterior\_sample(param, post, N)} can be used
to draw $N$ samples from the posterior. The arguments are similar to those described above. 

\vspace{.1in}
\textbf{Efficient correlation matrix operations.} The structure of the correlation functions used in the framework
can be exploited to derive efficient recursive algorithms for matrix-vector multiplication, matrix inversion, and
multiplication by the Cholesky factors of these matrices. These algorithms can make much larger data sizes
feasible. While these algorithms are not used in the main functions of the Matlab implementation of the framework,
reference code is provided in {\tt fastchol\_ar1.m}, {\tt fastchol\_eq.m}, {\tt fastmv\_ar1.m}, {\tt fastmv\_eq.m}, 
{\tt fastmv\_ar1inv.m}, and {\tt fastmv\_eqinv.m}. See the comments in those files for more information. 

\vspace{.1in}
\textbf{Alternative priors for $\phi$}. The only prior implemented for $\phi$ is the uniform prior above. This is
computed using Gaussian quadrature rules which are implemented in the file {\tt gauss\_quadrature.m}. That
file contains additional quadrature rules for placing a Gaussian-distribution prior on $\phi$ and a
Beta-distribution prior. While that code has been tested and appears to correctly implement the quadratures 
associated with those priors, the option to uses those priors is not currently included in the framework. This
could be changed, if desired, by making appropriate changes to the function {\tt eval\_beta\_lambda\_phi\_params} 
in {\tt eval\_posterior.m}. 

\vspace{.1in}
\textbf{Other information.} The above provides a basic outline of the verification framework. Additional information
on other optional features can be found by reading the code and its comments. 


\bibliographystyle{plain}
\bibliography{linver_manual}
\end{document}
